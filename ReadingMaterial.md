# Reading Material

We provide some basic reading material here. More detailed pointers can also be found in the slides. Feel free to contact us for more references. 

### Introduction to Language Modelling

Michael Repplinger , Lisa Beinborn , Willem Zuidema: <br>
[Vector-space models of words and sentences](http://www.nieuwarchief.nl/serie5/pdf/naw5-2018-19-3-167.pdf)

Daniel Jurafsky and James H. Martin: Speech and Language Processing (2020)  <br>
Chapter 3: [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf) <br>
Chapter 9: [Deep Learning Architectures for Sequence Processing](https://web.stanford.edu/~jurafsky/slp3/9.pdf)  <br>


### Neural Language Models
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer (2018): <br>
[Deep Contextualized Word Representations (ELMO)](https://www.aclweb.org/anthology/N18-1202/)

Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova (2019): <br>
[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://www.aclweb.org/anthology/N19-1423/)

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin (2017): <br>
[Attention is all you need (Transformer)](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

Tom B. Brown et al. (2020): <br>
[Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/abs/2005.14165)

### Illustrated Blog Posts
The original ELMO and Bert papers are hard to read. Jay Alammar provides very good conceptual introductions: 

[The illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)

[The illustrated BERT](http://jalammar.github.io/illustrated-bert/)


### Coding Tutorials
[A very basic  LSTM language model in Keras with Dutch training data](https://github.com/beinborn/ESSLLI2021/tree/main/code/lstm_example)

Kristina Gulordava: [An exmple LSTM language model in pytorch](https://github.com/facebookresearch/colorlessgreenRNNs/blob/master/src/language_models/model.py)

Chris McCormick: [Introductory tutorials to BERT](https://mccormickml.com/tutorials/)

Alexander Rush: [The annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)

### Our Research on Cognitive Plausibility

Samira Abnar, Lisa Beinborn, Rochelle Choenni, Willem Zuidema (Blackbox NLP 2019): <br>
[Blackbox Meets Blackbox: Representational Similarity and Stability Analysis of Neural Language Models and Brains](https://www.aclweb.org/anthology/W19-4820.pdf)

M. Giulianelli, J. Harding, F. Mohnert, D. Hupkes, W. Zuidema (Best Paper Blackbox NLP 2018): <br>
[Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information]()

Hollenstein & Beinborn (ACL 2021): <br>
[Relative Importance in Sentence Processing](https://arxiv.org/abs/2106.03471)

N. Hollenstein, F. Pirovano, C. Zhang, L. Jäger, L. Beinborn (NAACL 2021): <br>
[Multilingual Language Models Predict Human Reading Behavior](https://www.aclweb.org/anthology/2021.naacl-main.10.pdf)

Nora Hollenstein, Maria Barrett, Lisa Beinborn (LiNCR 2020): <br>
[Towards best practices for leveraging human language processing signals for natural language processing](https://aclanthology.org/2020.lincr-1.3/)

Lisa Beinborn, Samira Abnar, Rochelle Choenni (2019):<br>
[Robust Evaluation of Language–Brain Encoding Experiments](https://arxiv.org/abs/1904.02547)

